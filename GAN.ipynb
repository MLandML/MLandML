{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM3ZmBsU4eBOBe3Q1JOuGzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLandML/MLandML/blob/learning_projects/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mv4ZMuVT6Wv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys,os\n",
        "\n",
        "from tensorflow.keras.layers import Input,Dense,BatchNormalization,LeakyReLU,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "x_train,x_test = x_train/255.0,x_test/255.0\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "oGyBuy8ZgqrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten\n",
        "N,H,W = x_train.shape\n",
        "D = H*W\n",
        "x_train = x_train.reshape(-1,D)\n",
        "x_test = x_test.reshape(-1,D)"
      ],
      "metadata": {
        "id": "CuFkUAxGPr1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims = 100 #dimension of latent space\n",
        "\n",
        "def build_generator(latent_dims):\n",
        "  i = Input(shape=(latent_dims,))\n",
        "  x = Dense(512,activation=LeakyReLU(alpha=0.1))(i)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(512,activation=LeakyReLU(alpha=0.1))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(512,activation=LeakyReLU(alpha=0.1))(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = Dense(D,activation='tanh')(x)\n",
        "\n",
        "  model = Model(i,x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yqKLhU2cRbIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_size):\n",
        "  i = Input(shape=(img_size,))\n",
        "  x = Dense(512,activation=LeakyReLU(alpha=0.1))(i)\n",
        "  x = Dense(512,activation=LeakyReLU(alpha=0.1))(x)\n",
        "  x = Dense(1,activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(i,x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "hG_YCoxhU6C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = build_discriminator(D)\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer = Adam(0.0002,0.5),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "z = Input(shape=(latent_dims)) #random noise\n",
        "generator = build_generator(latent_dims)\n",
        "img = generator(z)\n",
        "#only the generator should be trained\n",
        "discriminator.trainable = False\n",
        "\n",
        "fake_pred = discriminator(img) #labeling fake images as real\n",
        "combined_model = Model(z,fake_pred) #combining generator and discriminator\n",
        "combined_model.compile(\n",
        "    loss = 'binary_crossentropy',\n",
        "    optimizer = Adam(0.0002,0.5)\n",
        ")\n",
        "\n",
        "#Train the GAN\n",
        "\n",
        "#Config\n",
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_period = 200 #saves some data after every mentioned no. of steps\n",
        "\n",
        "#batch labels to use when calling train_on_batch\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "#store losses\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "#create a folder to store generated images\n",
        "if not os.path.exists('gan_images'):\n",
        "  os.makedirs('gan_images')\n",
        "\n",
        "#a function to generate a grid of random samples from the generator and save them to a file\n",
        "def sample_images(epoch):\n",
        "  rows,columns=5,5\n",
        "  noise = np.random.randn(rows*columns,latent_dims)\n",
        "  imgs = generator.predict(noise)\n",
        "\n",
        "  #rescale the images\n",
        "  imgs = 0.5*imgs + 0.5\n",
        "  fig,axs = plt.subplots(rows,columns)\n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "    for j in range(columns):\n",
        "      axs[i,j].imshow(imgs[idx].reshape(H,W),cmap='gray')\n",
        "      axs[i,j].axis('off') #off so that we dont see lines in plots\n",
        "  fig.savefig('gan_images/%d.png'% epoch)\n",
        "  plt.close() #to clean up any resources"
      ],
      "metadata": {
        "id": "Cfbn1TSBiwfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  #Train discriminator\n",
        "\n",
        "  #select a batch of real images\n",
        "  idx = np.random.randint(0,x_train.shape[0],batch_size)\n",
        "  real_imgs = x_train[idx]\n",
        "\n",
        "  #select fake images\n",
        "  noise = np.random.randn(batch_size,latent_dims)\n",
        "  fake_imgs = generator.predict(noise)\n",
        "\n",
        "  d_real_loss,d_real_acc = discriminator.train_on_batch(real_imgs,ones)\n",
        "  d_fake_loss,d_fake_acc = discriminator.train_on_batch(fake_imgs,zeros)\n",
        "  d_loss = 0.5*(d_real_loss + d_fake_loss)\n",
        "  d_acc = 0.5*(d_real_acc + d_fake_acc)\n",
        "\n",
        "  #Train generator\n",
        "  noise = np.random.randn(batch_size,latent_dims)\n",
        "  g_loss = combined_model.train_on_batch(noise,ones)\n",
        "\n",
        "  #save the loss\n",
        "  d_losses.append(d_loss)\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"epoch: {epoch+1}/{epochs},\\\n",
        "         d_loss:{d_loss:2f},\\\n",
        "         g_loss:{g_loss:2f},\\\n",
        "         d_acc:{d_acc:2f}\"\n",
        "         )\n",
        "  if epoch % sample_period == 0:\n",
        "    sample_images(epoch)"
      ],
      "metadata": {
        "id": "JB2hLTVmieoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(g_losses,label='g_loss')\n",
        "plt.plot(d_losses,label='d_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "N4ON41q6UZYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gan_images"
      ],
      "metadata": {
        "id": "GH5W-h--UwUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imread\n",
        "a = imread('gan_images/0.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "BnM8mYxXXzXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/1000.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "8kHDH5iNYP0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/2000.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "DxCTV8dohjwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/5000.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "dtEX8PaRhnOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/10000.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "ILioRLC0hrru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/20000.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "TBNOqnl8h57E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = imread('gan_images/29800.png')\n",
        "plt.imshow(a)"
      ],
      "metadata": {
        "id": "M9pTbN-eh8pk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}